{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvolutionWithoutWeightTransport.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMqH0Gb4acu1vRjyxbq85QH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexCuozzo/SpatialFeatureLayer/blob/master/ConvolutionWithoutWeightTransport.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgBHUr_kKFlR",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Layer Without Weight Transport\n",
        "\n",
        "## What is weight transport?\n",
        "Weight transport is a common reason to reject a type of neural network model as being biologically implausible. It's any time that weights are reused or accessed across multiple layers, used multiple times within a layer, or are generally used in anything other than a local operation. The grounds for being able to claim biological implausibility is that biological neural networks don't have numbers floating around that can be used multiple places - everything is a physical connection that can only be modified by local information.\n",
        "\n",
        "\n",
        "## A step in the right direction\n",
        "\n",
        "It has been shown that the human visual cortex maintains a retinotopic map, meaning that visual imputs maintain spatial relationships as they are processed. The convolutional layer captures this behavior, as it constructs feature maps that align with the input. In addition, experiments like Hubel and Weisel have shown that there are feature selector neurons in the brain, and these features get more and more complex as they ascend the cortical hierarchy. Again, the convolutional layer captures this behavior with its use of filters. Not too bad. However, convolutional layers even still take advantage of weight transport, because they work through convolving a filter. The convolution operation is inherently nonlocal, so this is just a simple way of fixing it.\n",
        "\n",
        "\n",
        "\n",
        "## The proposal\n",
        "\n",
        "Similar to a convolutional layer, this layer consists of many \"filters\", but they don't move. They are wired up statically and updated just like a dense network, but nodes in a vertical stack all map to the same volume in the level below.\n",
        "\n",
        "I'll call it a SpatialFeature layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DooKap-VJ8eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import warnings\n",
        "from torch import Tensor\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE5l2Rj5mwRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
        "from typing import Optional, List, Tuple, Union, T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P5vHl7MKGKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpatialFeatureLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dimensions: Tuple[T, T, T],\n",
        "                 out_channels: int, \n",
        "                 kernel_size: Tuple[T, T], \n",
        "                 bias: bool = True,):\n",
        "        super(SpatialFeatureLayer, self).__init__()\n",
        "        self.in_channels, self.in_height, self.in_width = in_dimensions\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.output_height = self.in_height - kernel_size[0] + 1\n",
        "        self.output_width =  self.in_width - kernel_size[1] + 1\n",
        "        self.weight = Parameter(torch.Tensor(self.in_channels, self.in_height, self.in_width, self.out_channels, self.output_height, self.output_width))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(self.out_channels, self.output_height, self.output_width))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        init.kaiming_uniform_(self.bias, a=math.sqrt(5))\n",
        "        init.zeros_(self.weight)\n",
        "        for ow in range(self.weight.size(5)):\n",
        "          for oh in range(self.weight.size(4)):\n",
        "            for oc in range(self.weight.size(3)):\n",
        "              init.kaiming_uniform(self.weight[:, oh:oh+self.kernel_size[0], ow:ow+self.kernel_size[1], oc, oh, ow])\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = torch.tensordot(x, self.weight, dims=([1, 2, 3], [0, 1, 2]))\n",
        "      x = x + self.bias\n",
        "      return x\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLMK60ie5fo_",
        "colab_type": "text"
      },
      "source": [
        "# Training a model with the layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hctSfx75_wqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVooympj5hzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVR7Ry8g5nIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oibeKYU25qm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "515f2600-bc26-400a-c8f3-201d06478af8"
      },
      "source": [
        "data_path = \"./data\"\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=data_path, train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=data_path, train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzjRpG4g5tQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RegularConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RegularConvNet, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "            # Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(4096, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovSYIJBl6SgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SpatialDenseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialDenseNet, self).__init__()\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            SpatialFeatureLayer((3, 32, 32), 10, kernel_size=(3, 3)),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.ReLU(inplace=True),\n",
        "            SpatialFeatureLayer((10, 30, 30), 20, kernel_size=(3, 3)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Conv Layer block 2\n",
        "            SpatialFeatureLayer((20, 14, 14), 32, kernel_size=(3, 3)),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            SpatialFeatureLayer((32, 12, 12), 64, kernel_size=(3, 3)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "            # Conv Layer block 3\n",
        "            SpatialFeatureLayer((64, 5, 5), 128, kernel_size=(2, 2)),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            SpatialFeatureLayer((128, 4, 4), 256, kernel_size=(2, 2)),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(2304, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        x = self.conv_layer(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF10zp8B-IpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7aP27Vs8zWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "49f51f2e-d258-4edc-dd7a-f51a0c5ba273"
      },
      "source": [
        "spatial_dense = SpatialDenseNet().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lmc4m-iKSLI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3c95203-61e7-4312-c8c0-c68c1c901ea4"
      },
      "source": [
        "spatial_dense(inputs).size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 2304])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaXoAAcBI11O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ef657574-b294-4d22-872d-a3f66fb1d4c7"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "spatial_dense = spatial_dense.to(device)\n",
        "optimizer = torch.optim.Adam(spatial_dense.parameters(), lr=0.0003)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = spatial_dense(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.data.item()\n",
        "    running_loss /= len(trainloader)\n",
        "    print(\"Epoch: {0} | Loss: {1}\".format(epoch+1, running_loss))\n",
        "if not os.path.isdir('./checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "torch.save(spatial_dense.state_dict, \"./checkpoint/spatialDense\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Loss: 1.8816450436401855\n",
            "Epoch: 2 | Loss: 1.6222480977587688\n",
            "Epoch: 3 | Loss: 1.539194023975021\n",
            "Epoch: 4 | Loss: 1.4816871767153825\n",
            "Epoch: 5 | Loss: 1.4341203070357633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6rmLf6I83px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regular_conv = RegularConvNet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6MqWdQP-bO8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "7b813676-72e7-4c30-8f0e-f3a033cf3ffa"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "regular_conv = regular_conv.to(device)\n",
        "optimizer = torch.optim.Adam(regular_conv.parameters(), lr=0.0003)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = regular_conv(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.data\n",
        "    running_loss /= len(trainloader)\n",
        "    print(\"Epoch: {0} | Loss: {1}\".format(epoch+1, running_loss))\n",
        "if not os.path.isdir('./checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "torch.save(regular_conv.state_dict, \"./checkpoint/regularConv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-d3d4f3bdf73d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mregular_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregular_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregular_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'regular_conv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKfT7o4KHK-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.sl = SpatialFeatureLayer((3, 32, 32), 5, (3, 3))\n",
        "        self.fc = nn.Linear(5*30*30, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.sl(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izglEt2xB-5-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c61ce546-cf42-4f13-fd57-7b4fffaa3803"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "model = SimpleModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader, 0):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.data.item()\n",
        "    running_loss /= len(trainloader)\n",
        "    print(\"Epoch: {0} | Loss: {1}\".format(epoch+1, running_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Loss: 2.694465982639576\n",
            "Epoch: 2 | Loss: 2.0974156254392757\n",
            "Epoch: 3 | Loss: 2.073304990673309\n",
            "Epoch: 4 | Loss: 2.076695164451209\n",
            "Epoch: 5 | Loss: 2.074775897328506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2xDzDvaAAWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # test model\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model.forward(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "        print(\"Accuracy: {} %\".format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voXl3HIjB2lI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "754b749f-42f3-40eb-dfb4-343632b4a8ce"
      },
      "source": [
        "test_model(regular_conv, testloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 86.9 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPfKXFyCB5Y5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b57eeafe-fddc-4c2d-f2b7-3e164754c8fa"
      },
      "source": [
        "test_model(spatial_dense, testloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 52.73 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQpb8ClUOYFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGkpcQsy2U7z",
        "colab_type": "text"
      },
      "source": [
        "# Take aways\n",
        "\n",
        "\n",
        "This is biologically plausible, but not very useful. This definitely needs to be more optimized for memory as it is currently very intensive. As far as machine learning methods go, it works, but probably not as well as other methods like the Conv2d layer. The reason is that there are just so many more weights to tune, so training takes a while. From a practical standpoint, this is probably not recommended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmSUi55I2uNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}